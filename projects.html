<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Projects - Bruno Scholles</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <nav class="sticky-nav">
        <div class="nav-center">
            <a href="index.html" class="nav-icon" aria-label="Home" data-tooltip="Home">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.6" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true">
                    <path d="M2.5 11.75L11.63 3.2a.75.75 0 0 1 .98 0l9.14 8.55"></path>
                    <path d="M5 10.25V21h5.5v-5.5h3V21H19V10.25"></path>
                </svg>
            </a>
            <a href="projects.html" class="nav-icon" aria-label="Projects" data-tooltip="Projects">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true">
                    <polygon points="12 2 2 7 12 12 22 7 12 2"></polygon>
                    <polyline points="2 12 12 17 22 12"></polyline>
                    <polyline points="2 17 12 22 22 17"></polyline>
                </svg>
            </a>
            <a href="https://github.com/BrunoScholles98" target="_blank" rel="noopener noreferrer" class="nav-icon" aria-label="GitHub" data-tooltip="GitHub">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor">
                    <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
                </svg>
            </a>
            <a href="https://www.linkedin.com/in/bruno-scholles/" target="_blank" rel="noopener noreferrer" class="nav-icon" aria-label="LinkedIn" data-tooltip="LinkedIn">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor">
                    <path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/>
                </svg>
            </a>
            <a href="publications.html" class="nav-icon" aria-label="Publications" data-tooltip="Publications">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <path d="M4 19.5A2.5 2.5 0 0 1 6.5 17H20"></path>
                    <path d="M6.5 2H20v20H6.5A2.5 2.5 0 0 1 4 19.5v-15A2.5 2.5 0 0 1 6.5 2z"></path>
                </svg>
            </a>
            <a href="photography.html" class="nav-icon" aria-label="Photography" data-tooltip="Photography">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <path d="M23 19a2 2 0 0 1-2 2H3a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h4l2-3h6l2 3h4a2 2 0 0 1 2 2z"></path>
                    <circle cx="12" cy="13" r="4"></circle>
                </svg>
            </a>
        </div>
    </nav>

    <main class="terminal-container">
        <section class="terminal-box">
            <div class="terminal-title-bar">
                <span class="terminal-btn red"></span>
                <span class="terminal-btn yellow"></span>
                <span class="terminal-btn green"></span>
            </div>
            <div class="terminal-body">
                <header class="projects-header">
                    <div>
                        <h1 class="page-title">Main Projects</h1>
                        <p class="intro-text projects-header__subtitle">
                            Explore my flagship public initiatives below; for the complete list of repositories, visit my GitHub profile.
                        </p>
                    </div>
                    <a href="https://github.com/BrunoScholles98" target="_blank" rel="noopener noreferrer" class="project-link gh-link projects-header__cta">
                        View GitHub Profile
                    </a>
                </header>
                <div class="section-divider"></div>

                <div class="projects-accordion">
                    <details class="project-item">
                        <summary>
                            <h2>GigaSistêmica — Systemic Health Diagnostics with AI</h2>
                            <span class="summary-icon" aria-hidden="true"></span>
                        </summary>
                        <div class="project-content project-content--stacked">
                            <div class="project-description">
                                <p>
                                    GigaSistêmica is an open science programme that blends panoramic dental imaging, radiomics, and specialised language models to deliver systemic health diagnostics from data acquisition to clinical reporting. Each research stream releases curated datasets, training notebooks, and deployment artefacts so partners can reproduce the pipelines in hospitals and labs.
                                </p>
                                <div class="project-actions">
                                    <a href="https://github.com/GIGASISTEMICA/GigaSistemica-Advancing-Systemic-Health-Diagnostics-through-AI" target="_blank" rel="noopener noreferrer" class="project-link gh-link">
                                        View umbrella repository
                                    </a>
                                    <a href="https://github.com/GIGASISTEMICA/GigaSistemica-Advancing-Systemic-Health-Diagnostics-through-AI/blob/main/README.md" target="_blank" rel="noopener noreferrer" class="project-link">
                                        Read project manifesto
                                    </a>
                                </div>
                            </div>

                            <div class="project-subsection">
                                <figure class="subsection-figure">
                                    <img src="ProjectImgs/68747470733a2f2f692e6962622e636f2f674d4239353250352f4f7374656f4d652e706e67.png" alt="Grad-CAM overlay highlighting mandibular cortex regions considered by EfficientNet during osteoporosis screening" loading="lazy" decoding="async">
                                </figure>
                                <div class="subsection-body">
                                    <h3>Osteoporosis Screening with EfficientNet</h3>
                                    <p>
                                        Peer-reviewed in <em>Biomedical Signal Processing and Control</em>, this study curates 750 panoramic radiographs (579 C1, 171 C3) to fine-tune EfficientNet-B5/B6/B7 across full images and mandibular cortex crops. Cross-validation delivers 95–99 % accuracy, while Grad-CAM maps verify attention along the inferior cortex. A DXA-referenced cohort of 60 adults showed the CNNs outperforming general dentists and approaching radiologist accuracy, with the entire preprocessing and training stack published for replication.
                                    </p>
                                    <ul class="subsection-highlights">
                                        <li>K-fold validation across two input strategies with precision and recall consistently above 0.95.</li>
                                        <li>DXA comparison confirms the models’ clinical utility for early screening referrals.</li>
                                        <li>Transparent pipeline: CVAT annotations, rolling-ball denoising, Grad-CAM auditing, and open-source code.</li>
                                    </ul>
                                    <div class="subsection-actions">
                                        <a href="https://github.com/BrunoScholles98/Deep-Learning-for-Bone-Health-Classification-through-X-ray-Imaging" target="_blank" rel="noopener noreferrer" class="project-link">
                                            Open osteoporosis repository
                                        </a>
                                    </div>
                                </div>
                            </div>

                            <div class="project-subsection">
                                <figure class="subsection-figure">
                                    <img src="ProjectImgs/68747470733a2f2f692e6962622e636f2f53584759343533582f417465726f6d612e706e67.png" alt="Panoramic radiograph showing attention-guided localisation of carotid atheroma calcifications" loading="lazy" decoding="async">
                                </figure>
                                <div class="subsection-body">
                                    <h3>Carotid Atheroma Hybrid Detection & Segmentation</h3>
                                    <p>
                                        A 694-image dataset (372 positive CACs) underpins a hybrid FastViT, AttentionNet, and DC-UNet workflow that flags, localises, and segments carotid artery calcifications directly from routine dental exams. Bounding-box proposals focus on the C3–C4 window, while mask consensus quantifies morphology with precision, recall, and specificity above 0.90—fostering opportunistic cardiovascular screening in dental settings.
                                    </p>
                                    <ul class="subsection-highlights">
                                        <li>Bounding-box and pixel-level annotations curated with CVAT and validated by oral radiologists.</li>
                                        <li>FastViT classifiers trigger detection; AttentionNet refines ROIs; DC-UNet produces clinical-grade masks.</li>
                                        <li>Reduces false positives via mask–box fusion, supporting longitudinal monitoring of calcification load.</li>
                                    </ul>
                                    <div class="subsection-actions">
                                        <a href="https://github.com/BrunoScholles98/Deep-Learning-for-Bone-Health-Classification-through-X-ray-Imaging" target="_blank" rel="noopener noreferrer" class="project-link">
                                            Open CAC repository
                                        </a>
                                    </div>
                                </div>
                            </div>

                            <div class="project-subsection">
                                <figure class="subsection-figure">
                                    <img src="ProjectImgs/MainPage_Example.png" alt="GigaXReport web interface illustrating language selection, prompt input, and X-ray upload" loading="lazy" decoding="async">
                                </figure>
                                <div class="subsection-body">
                                    <h3>GigaXReport Multimodal Diagnostic Framework</h3>
                                    <p>
                                        The flagship Flask application unifies the research outputs into a clinician-facing workflow. Users upload panoramic radiographs, add clinical prompts, and receive interactive PDF reports that combine EfficientNet osteoporosis risk scores, FastViT/Faster R-CNN/DC-UNet atheroma outputs, and MedGemma-4B bilingual narratives grounded in radiomics metrics.
                                    </p>
                                    <ul class="subsection-highlights">
                                        <li>MedGemma small language model fine-tuned for Portuguese/English radiology discourse.</li>
                                        <li>Automated Grad-CAM overlays, segmentation visualisations, and structured report sections (“Bone Health”, “Atheroma Diagnosis”).</li>
                                        <li>Container-ready stack with environment scripts, prompt templates, and CLI helpers for hospital deployment.</li>
                                    </ul>
                                    <div class="subsection-actions">
                                        <a href="https://github.com/BrunoScholles98/GigaXReport-A-Multimodal-Diagnostic-Framework-with-Specialized-Language-Models-SLMs/tree/main" target="_blank" rel="noopener noreferrer" class="project-link">
                                            Open GigaXReport repository
                                        </a>
                                    </div>
                                </div>
                            </div>

                        </div>
                    </details>

                    <details class="project-item">
                        <summary>
                            <h2>Learned Gunshot Wound Identification</h2>
                            <span class="summary-icon" aria-hidden="true"></span>
                        </summary>
                        <div class="project-content">
                            <div class="project-media">
                                <figure>
                                    <img src="ProjectImgs/Blur_arqu_wounds.png" alt="Data pipeline diagram illustrating the gunshot wound classification workflow" loading="lazy" decoding="async">
                                    <figcaption>Dataset curation and deep learning pipeline for forensic wound classification.</figcaption>
                                </figure>
                            </div>
                            <div class="project-description">
                                <p class="alert-sensitive">
                                    This project contains sensitive photographic material of real gunshot wounds, which some viewers may find disturbing. Viewer discretion is advised.
                                </p>
                                <p>
                                    Developed in collaboration with the Civil Police of the Federal District, this forensic initiative evaluates 2,551 real-world wound images to classify entry vs. exit wounds and medico-legal shooting distance. Experiments spanned 59 TorchVision architectures, where ResNet152 achieved 86.9 % accuracy for wound type and 92.5 % accuracy for shooting-distance categories using extensive augmentation, SMOTE+ENN resampling, and stratified cross-validation.
                                </p>
                                <p>
                                    Results were published in the <em>International Journal of Legal Medicine</em>, highlighting the model’s robustness despite imbalanced classes and variable capture conditions, and advocating for AI as an assistive tool that complements forensic expertise.[2]
                                </p>
                                <ul class="project-highlights">
                                    <li>Real crime-scene dataset (2012–2022) with manual cropping, labelling, and augmentation workflows to standardize forensic wound imagery.</li>
                                    <li>Comparative benchmarking of 59 CNN and transformer models, identifying ResNet152 as the most stable architecture across k-fold validation.</li>
                                    <li>Open-source tooling on GitLab documents preprocessing, training, and evaluation pipelines for reproducibility.</li>
                                </ul>
                                <div class="project-actions">
                                    <a href="https://gitlab.com/lisa-unb/leguwoi" target="_blank" rel="noopener noreferrer" class="project-link gh-link">
                                        Open wound classification repository
                                    </a>
                                </div>
                            </div>
                        </div>
                    </details>

                    <details class="project-item">
                        <summary>
                            <h2>Amazon Rainforest Wildfire Typification</h2>
                            <span class="summary-icon" aria-hidden="true"></span>
                        </summary>
                        <div class="project-content">
                            <div class="project-media">
                                <figure>
                                    <img src="ProjectImgs/Amazon.png" alt="Satellite map with colour-coded wildfire typification outputs" loading="lazy" decoding="async">
                                    <figcaption>Spatial typification map generated from MODIS/VIIRS datasets.</figcaption>
                                </figure>
                            </div>
                            <div class="project-description">
                                <p>
                                    This research-grade monitoring stack distinguishes anthropogenic and natural wildfire events across Brazil’s Amazon Legal. The project consolidates multi-year datasets from MODIS, VIIRS, and DETER with environmental covariates, building feature stores that feed geospatial Random Forest and MLP ensembles to assist CENSIPAM analysts with rapid situational awareness.
                                </p>
                                <ul class="project-highlights">
                                    <li>Automated ingestion and harmonisation of satellite fire alerts, atmospheric indicators, and land-use layers for long-term trend analysis.</li>
                                    <li>Feature-engineering toolkit that encodes spatiotemporal context (biome, protected area, weather) before training supervised classifiers.</li>
                                    <li>Explainability reports that highlight the drivers behind each typification decision to support accountability in public policy.</li>
                                    <li>Deployment-ready notebooks and Dockerised pipelines for nightly retraining and map generation.</li>
                                </ul>
                                <div class="project-actions">
                                    <a href="https://github.com/BrunoScholles98/Machine-Learning-Based-Typification-System-of-Forest-Fires-in-Amazon-Rainforest" target="_blank" rel="noopener noreferrer" class="project-link gh-link">
                                        View repository
                                    </a>
                                    <a href="https://github.com/BrunoScholles98/Machine-Learning-Based-Typification-System-of-Forest-Fires-in-Amazon-Rainforest/blob/main/README.md" target="_blank" rel="noopener noreferrer" class="project-link">
                                        Read technical report
                                    </a>
                                </div>
                            </div>
                        </div>
                    </details>

                    <details class="project-item">
                        <summary>
                            <h2>FOVQA — Framework for Objective Visual Quality Assessment</h2>
                            <span class="summary-icon" aria-hidden="true"></span>
                        </summary>
                        <div class="project-content">
                            <div class="project-media">
                                <figure>
                                    <img src="https://opengraph.githubassets.com/1/BrunoScholles98/Framework-for-Objective-Visual-Quality-Assessment-FOVQA" alt="Open Graph preview of the FOVQA framework repository" loading="lazy" decoding="async">
                                    <figcaption>Repository overview for FOVQA.</figcaption>
                                </figure>
                            </div>
                            <div class="project-description">
                                <p>
                                    FOVQA is a modular benchmarking suite that unifies classical objective metrics and deep neural predictors to estimate mean opinion scores (MOS) for video quality assessment. The framework streamlines dataset preparation, experiment orchestration, and comparative reporting so teams can pit handcrafted metrics (PSNR, SSIM, VMAF, LPIPS) against CNN and Transformer-based regressors.
                                </p>
                                <ul class="project-highlights">
                                    <li>Plug-and-play pipelines for computing metric baselines and training custom PyTorch models on curated VQA datasets.</li>
                                    <li>Comprehensive evaluation dashboards with cross-metric correlations and MOS residual analysis.</li>
                                    <li>Support for mixed precision training, experiment tracking, and GPU-efficient data loaders.</li>
                                    <li>Reusable notebooks that document research published at SIBGRAPI and related venues.</li>
                                </ul>
                                <div class="project-actions">
                                    <a href="https://github.com/BrunoScholles98/Framework-for-Objective-Visual-Quality-Assessment-FOVQA" target="_blank" rel="noopener noreferrer" class="project-link gh-link">
                                        View repository
                                    </a>
                                    <a href="https://github.com/BrunoScholles98/Framework-for-Objective-Visual-Quality-Assessment-FOVQA/blob/main/README.md" target="_blank" rel="noopener noreferrer" class="project-link">
                                        Read methodology guide
                                    </a>
                                </div>
                            </div>
                        </div>
                    </details>

                </div>
            </div>
        </section>
    </main>

    <footer class="site-footer">
        © 2025 Bruno Scholles — All Rights Reserved.
    </footer>
</body>
</html>

